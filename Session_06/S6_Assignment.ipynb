{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S6_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMayfnRYxQr9GrNBP2BdHOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkshitAggarwal/TSAI_EVA5B2_Phase1/blob/main/Session_06/S6_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNUwjZcK0loy"
      },
      "source": [
        "### 1. Call all necessary libraries. \n",
        "\n",
        " Only call libraries and dependencies which are used in the notebook, because each library takes up additional memory. \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IypAnXwfGGfK"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LstFsihC0-1n"
      },
      "source": [
        "### 2. Create data transform compose. \n",
        "\n",
        "Use the compose function from `torch.transforms` to compile all necessarry data transformations that you want to apply to your datasets in one place. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Q4H4EGIekU"
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                      transforms.RandomRotation((-8.0, 8.0), fill=(1,)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                      ])\n",
        "test_transforms = transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                    ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34vOWr4G1bAR"
      },
      "source": [
        "### 3. Create datasets. \n",
        "\n",
        "Create your `train`, `validate`, `test` datasets and apply the transforms that were created above using compose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqZgWfbXanAO"
      },
      "source": [
        "train_dataset = datasets.MNIST(root='./data', train = True, transform = train_transforms, download = True)\n",
        "test_dataset = datasets.MNIST(root='./data', train = False, transform = test_transforms, download = True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzv-My332X8f"
      },
      "source": [
        "### 4. Create dataloaders. \n",
        "\n",
        "Create dataloaders for respective datasets. Dataloaders are iterable objects that can be fed to the model in smaller, randomized batches to avoid overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dCxLGH2dswi"
      },
      "source": [
        "SEED = 1\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\") #Assigns device based on availability\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "\n",
        "#Arguments to be fed into dataloaders. \n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "##Train and Test dataloaders. \n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, **dataloader_args)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, **dataloader_args)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD4-aZ9f1ouW"
      },
      "source": [
        "### 5. Data statistics.\n",
        "\n",
        "Check all necessary data statistics, such as min, max, std, mean values of the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO8p44SO3PKK"
      },
      "source": [
        "### 6. Visualize dataset. \n",
        "\n",
        "Take a look at a few images of the training dataset. It gives you an idea of what you're dealing with. By looking at the dataset examples you can determine how difficult your task can be and what kind of transformations may help your model predict better. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r4vzX0P3nB2"
      },
      "source": [
        "### 7. Define model architecture. \n",
        "\n",
        "Create your model class and define model architecture. Keep in mind things like \n",
        "\n",
        "\n",
        "1. Model complexity\n",
        "2. Number of layers\n",
        "3. Parameters\n",
        "4. Padding in Conv Layers\n",
        "5. Kernel size\n",
        "6. Receptive field\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVzP-nozc6oj"
      },
      "source": [
        "# Ghost Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffga8ywnc6Hn"
      },
      "source": [
        "class GBN(nn.Module):\n",
        "  def __init__(self, num_features, groups, eps=1e-05):\n",
        "    super(GBN, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.eps = eps\n",
        "    self.groups = groups\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    X: Input Tensor with (M, C, F) dimensions\n",
        "    groupsM: Number of groups for the mini-batch dimension\n",
        "    eps: A small value to prevent division by zero\n",
        "    \"\"\"\n",
        "    # Split the mini-batch dimension into groups of smaller batches\n",
        "    M, C, x, y = X.shape\n",
        "    X = X.reshape(self.groups, -1, C, x, y)\n",
        "    # Calculate statistics over dim(0) x dim(2) number\n",
        "    # of slices of dim(1) x dim(3) dimension each\n",
        "    mean = X.mean([1, 3], keepdim=True)\n",
        "    var = X.var([1, 3], unbiased=False, keepdim=True)\n",
        "    # Normalize X\n",
        "    X = (X - mean) / (torch.sqrt(var + self.eps))\n",
        "    # Reshape into the initial tensor shape\n",
        "    X = X.reshape(M, C, x, y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elA_SSnLm6WW"
      },
      "source": [
        "\n",
        "# New Conv Block\n",
        "\n",
        "1. Convolution Layer\n",
        "2. RelU\n",
        "3. BatchNorm2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hrhcfFMm5xp"
      },
      "source": [
        "def convBlock(in_channels, out_channels, kernel_size, **kwargs):\n",
        "    batch_normalize = nn.ModuleDict([\n",
        "                ['BN', nn.BatchNorm2d(out_channels)],\n",
        "                ['GBN', GBN(out_channels, 10)],\n",
        "                ['None', None]])\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, bias = False),\n",
        "                          batch_normalize[kwargs['batch_norm']],\n",
        "                          nn.ReLU()]\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2dWAB-AE0iY"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    #Constructor function initializes an object of Net() and defines all the layers. \n",
        "    #Each layer is defined here separately and aren't connected to each other in any way yet. \n",
        "    def __init__(self, **kwargs):\n",
        "        super(Net, self).__init__()\n",
        "        self.convBlock1 = convBlock(1, 8, kernel_size = 3, **kwargs) #in_channel: 1,28,28; out_channel: 8,26,26; RF: 3\n",
        "        self.convBlock2 = convBlock(8, 8, kernel_size = 3, **kwargs) #in_channel: 8,26,26; out_channel: 8,24,24; RF: 5\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #in_channel: 8,24,24; out_channel: 8,12,12; RF: 10\n",
        "        self.convBlock3 = convBlock(8, 16, kernel_size = 3, **kwargs) #in_channel: 8,12,12; out_channel: 16,10,10; RF: 12\n",
        "        self.convBlock4 = convBlock(16, 16, kernel_size = 3, **kwargs) #in_channel: 16,10,10; out_channel: 16,8,8; RF: 14\n",
        "        self.convBlock5 = convBlock(16, 32, kernel_size = 3, **kwargs) #in_channel: 16,8,8; out_channel: 32,6,6; RF: 16\n",
        "        self.gap = nn.AvgPool2d((6, 6)) #in_channel: 32,6,6; out_channel: 32,1,1; RF: ?\n",
        "        self.convBlock6 = convBlock(32, 20, kernel_size = 1, **kwargs)#in_channels: 32,1,1; out_channels: 20,1,1\n",
        "        self.convBlock7 = nn.Conv2d(20, 10, 1, bias = False) #in_channel: 20,1,1; out_channel: 10,1,1; \n",
        "\n",
        "    #Forward function takes an object and it passes through each layer sequentially. \n",
        "    def forward(self, x):\n",
        "        x = self.convBlock1(x)\n",
        "        x = self.convBlock2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convBlock3(x)\n",
        "        x = self.convBlock4(x)\n",
        "        x = self.convBlock5(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convBlock6(x)\n",
        "        x = self.convBlock7(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim = -1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZTMm2D_4KxX"
      },
      "source": [
        "### 8. Look at model summary. \n",
        "\n",
        "Use `torchsummary` to look at model summary, load the model on gpu if available. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAIyfFhpE5zi",
        "outputId": "5524c915-1e17-4473-94ae-bf7fb1c94ab5"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "model = Net(batch_norm='BN').to(device) #Converts our model into the respective device.\n",
        "summary(model, input_size=(1, 28, 28)) #Prints the summary of our model based on an input size."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "       BatchNorm2d-2            [-1, 8, 26, 26]              16\n",
            "              ReLU-3            [-1, 8, 26, 26]               0\n",
            "            Conv2d-4            [-1, 8, 24, 24]             576\n",
            "       BatchNorm2d-5            [-1, 8, 24, 24]              16\n",
            "              ReLU-6            [-1, 8, 24, 24]               0\n",
            "         MaxPool2d-7            [-1, 8, 12, 12]               0\n",
            "            Conv2d-8           [-1, 16, 10, 10]           1,152\n",
            "       BatchNorm2d-9           [-1, 16, 10, 10]              32\n",
            "             ReLU-10           [-1, 16, 10, 10]               0\n",
            "           Conv2d-11             [-1, 16, 8, 8]           2,304\n",
            "      BatchNorm2d-12             [-1, 16, 8, 8]              32\n",
            "             ReLU-13             [-1, 16, 8, 8]               0\n",
            "           Conv2d-14             [-1, 32, 6, 6]           4,608\n",
            "      BatchNorm2d-15             [-1, 32, 6, 6]              64\n",
            "             ReLU-16             [-1, 32, 6, 6]               0\n",
            "        AvgPool2d-17             [-1, 32, 1, 1]               0\n",
            "           Conv2d-18             [-1, 20, 1, 1]             640\n",
            "      BatchNorm2d-19             [-1, 20, 1, 1]              40\n",
            "             ReLU-20             [-1, 20, 1, 1]               0\n",
            "           Conv2d-21             [-1, 10, 1, 1]             200\n",
            "================================================================\n",
            "Total params: 9,752\n",
            "Trainable params: 9,752\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.33\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.37\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c7B6fX6Fk7Y"
      },
      "source": [
        "### 9. Define Train and Test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZwzHIFYE_6V"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, L1):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # get samples\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Init\n",
        "        optimizer.zero_grad()\n",
        "        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "        # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "        # Predict\n",
        "        y_pred = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "\n",
        "        if L1 is True:\n",
        "            lambda_l1 = 0.001\n",
        "            loss = mse(y_pred, target)\n",
        "            l = 0\n",
        "            for p in model.parameters():\n",
        "                l += p.abs().sum()\n",
        "            loss += lambda_l1 * l\n",
        "        else: \n",
        "            loss = F.nll_loss(y_pred, target)\n",
        "\n",
        "        train_losses.append(loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update pbar-tqdm\n",
        "        \n",
        "        pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "        description = 'Loss={:.4f} Batch_id={} Accuracy={:.2f}'.format(loss.item(), batch_idx, 100*correct/processed)\n",
        "        train_acc.append(100*correct/processed)\n",
        "    print(description)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    misclassified_images = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            if len(misclassified_images) < 25:\n",
        "                for d, t, p in zip(data, target, pred):\n",
        "                    if t is not p:\n",
        "                        misclassified_images.append((d, t, p))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        accuracy))\n",
        "    \n",
        "    test_acc.append(accuracy)\n",
        "    return test_loss, accuracy, misclassifed_images"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buXmBFITFojT"
      },
      "source": [
        "### 10. Define optimizer and train the model. \n",
        "\n",
        "Train the model for n epochs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59wloJGSA5K"
      },
      "source": [
        "### Train and Test utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZZVfsaqKvi_"
      },
      "source": [
        "def optimizer_pick(L2 = False):\n",
        "    return optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay = 0.001) if L2 is True else optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "l1_reg = [True, False, True, False, True]\n",
        "\n",
        "optimizers = [optimizer_pick(L2 = False), optimizer_pick(L2 = True), optimizer_pick(L2 = True), optimizer_pick(L2 = False), optimizer_pick(L2 = True)]\n",
        "\n",
        "models = [\n",
        "          Net(batch_norm = 'BN').to(device),\n",
        "          Net(batch_norm = 'BN').to(device),\n",
        "          Net(batch_norm = 'BN').to(device),\n",
        "          Net(batch_norm = 'GBN').to(device),\n",
        "          Net(batch_norm = 'GBN').to(device),\n",
        "        ]\n",
        "labels = [\n",
        "          'with L1 + BN',\n",
        "          'with L2 + BN',\n",
        "          'with L1 + L2 + BN',\n",
        "          'with GBN',\n",
        "          'with L1 + L2 + GBN']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x39Ry2lGSHDT"
      },
      "source": [
        "### Training and Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "Tc-NyYtQ1o2C",
        "outputId": "88100a71-f15a-4c16-851c-48fcf1085977"
      },
      "source": [
        "help(mse())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6f73616d05fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'mse' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "W4sDZ9QyR8g1",
        "outputId": "4a049433-16be-4fb2-d09a-52ddd633ae0c"
      },
      "source": [
        "model_accuracies = [] #Accuracies over different models\n",
        "model_losses = [] #Losses over different models\n",
        "all_misclassified = [] \n",
        "\n",
        "for model, l1, optimizer, label in zip(models, l1_reg, optimizers, labels):\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=1)\n",
        "    print('---------MODEL:', label ,'---------')\n",
        "    epoch_losses = [] #Keep track of losses per epoch in current model\n",
        "    epoch_accuracies = [] #Keep track of accuracies per epoch in current model \n",
        "    EPOCHS = 25\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(\"EPOCH:\", epoch)\n",
        "        print('Learning rate: ', optimizer.param_groups[0]['lr'])\n",
        "        train(model, device, train_loader, optimizer, epoch, L1=l1)\n",
        "        loss, accuracy, misclassified = test(model, device, test_loader)\n",
        "        scheduler.step(loss)\n",
        "        epoch_losses.append(loss)\n",
        "        epoch_accuracies.append(accuracy)\n",
        "    model_losses.append(epoch_losses)\n",
        "    model_accuracies.append(epoch_accuracies)\n",
        "    all_misclassified.append(misclassified)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------MODEL: with L1 + BN ---------\n",
            "EPOCH: 0\n",
            "Learning rate:  0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ba66e59e83a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Learning rate: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmisclassified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-4af1df0233e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, L1)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mL1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlambda_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mse' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPBPrxGPFx-f"
      },
      "source": [
        "### 11. Visualize model performance\n",
        "\n",
        "Use matplotlib to plot the model summary to directly compare results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw0hQiKvK5Xo"
      },
      "source": [
        "fig, axs = plt.subplots(2,2,figsize=(20,15))\n",
        "axs[0, 0].plot(train_losses)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[1, 0].plot(train_acc)\n",
        "axs[1, 0].set_title(\"Training Accuracy\")\n",
        "axs[0, 1].plot(test_losses)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 1].plot(test_acc)\n",
        "axs[1, 1].set_title(\"Test Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olW7YU8fX3xY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}